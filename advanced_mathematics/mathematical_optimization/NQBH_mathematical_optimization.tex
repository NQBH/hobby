\documentclass[oneside]{book}
\usepackage[backend=biber,natbib=true,style=authoryear]{biblatex}
\addbibresource{/home/hong/1_NQBH/reference/bib.bib}
\usepackage[vietnamese,english]{babel}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=red,citecolor=magenta]{hyperref}
\usepackage{amsmath,amssymb,amsthm,mathtools,float,graphicx}
\allowdisplaybreaks
\numberwithin{equation}{section}
\newtheorem{assumption}{Assumption}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{example}{Example}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{notation}{Notation}[chapter]
\newtheorem{principle}{Principle}[chapter]
\newtheorem{problem}{Problem}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{question}{Question}[chapter]
\newtheorem{remark}{Remark}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\usepackage[left=0.5in,right=0.5in,top=1.5cm,bottom=1.5cm]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{\small \textsc{Sect.} ~\thesection}
\rhead{\small \nouppercase{\leftmark}}
\renewcommand{\sectionmark}[1]{\markboth{#1}{}}
\cfoot{\thepage}
\def\labelitemii{$\circ$}

\title{Some Topics in Mathematical Optimization}
\author{\selectlanguage{vietnamese} Nguyễn Quản Bá Hồng\footnote{Independent Researcher, Ben Tre City, Vietnam\\e-mail: \texttt{nguyenquanbahong@gmail.com}}}
\date{\today}

\begin{document}
\maketitle
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\tableofcontents

\chapter*{Foreword}

A collection of \& some personal notes on Mathematical Optimization, especially the 3 major topics: Optimal Control, Shape Optimization, \& Topology Optimization.

\textbf{Keywords.} Optimal control; Shape optimization; Topology optimization.

%------------------------------------------------------------------------------%

\chapter{Wikipedia's}

\section{\href{https://en.wikipedia.org/wiki/Shape_optimization}{Wikipedia\texttt{/}Shape Optimization}}
``\textit{Shape optimization} is part of the field of \href{https://en.wikipedia.org/wiki/Optimal_control}{optimal control} theory. The typical problem is to find the \href{https://en.wikipedia.org/wiki/Shape}{shape} which is optimal in that it minimizes a certain cost \href{https://en.wikipedia.org/wiki/Functional_(mathematics)}{functional} while satisfying given \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{constraints}. In many cases, the functional being solved depends on the solution of a given \href{https://en.wikipedia.org/wiki/Partial_differential_equation}{PDE} defined on the variable domain.

\href{https://en.wikipedia.org/wiki/Topology_optimization}{Topology optimization} is, in addition, concerned with the number of connected components\texttt{/}boundaries belonging to the domain. Such methods are needed since typically shape optimization methods work in a subset of allowable shapes which have fixed topological properties, such as having a fixed number of holes in them. Topological optimization techniques can then help work around the limitations of pure shape optimization.'' -- \href{https://en.wikipedia.org/wiki/Shape_optimization}{Wikipedia\texttt{/}shape optimization}

\subsection{Definition}
``Mathematically, shape optimization can be posed as the problem of finding a \href{https://en.wikipedia.org/wiki/Bounded_set}{bounded set} $\Omega$, \href{https://en.wikipedia.org/wiki/Maxima_and_minima}{minimizing} a \href{https://en.wikipedia.org/wiki/Functional_(mathematics)}{functional} $\mathcal{F}(\Omega)$, possibly subject to a \href{https://en.wikipedia.org/wiki/Constraint_(mathematics)}{constraint} of the form $\mathcal{G}(\Omega) = 0$. Usually we are interested in sets $\Omega$ which are \href{https://en.wikipedia.org/wiki/Lipschitz_continuity}{Lipschitz} or $C^1$ \href{https://en.wikipedia.org/wiki/Boundary_(topology)}{boundary} \& consist of finite many \href{https://en.wikipedia.org/wiki/Connected_component_(analysis)}{components}, which is a way of saying that we would like to find a rather pleasing shape as a solution, \fbox{not some jumble of rough bits \& pieces}. Sometimes additional constraints need to be imposed to that end to ensure well-posedness of the problem \& uniqueness of the solution.

Shape optimization is an \href{https://en.wikipedia.org/wiki/Infinite-dimensional_optimization}{infinite-dimensional optimization} problem. Furthermore, the space of allowable shapes over which the optimization is performed does not admit a \href{https://en.wikipedia.org/wiki/Vector_space}{vector space} structure, making application of traditional optimization methods more difficult.'' -- \href{https://en.wikipedia.org/wiki/Shape_optimization#Definition}{Wikipedia\texttt{/}shape optimization\texttt{/}definition}

\subsection{Examples}
\begin{itemize}
	\item ``among all 3D shapes of given volume, find the one which has minimal surface area. Here: $\mathcal{F}(\Omega) = \operatorname{Area}(\partial\Omega)$, with $\mathcal{G}(\Omega) = \operatorname{Volume}(\Omega) = {\rm const}$. The answer, given by the \href{https://en.wikipedia.org/wiki/Isoperimetric_inequality}{isoperimetric inequality}, is a \href{https://en.wikipedia.org/wiki/Ball_(mathematics)}{ball}.
	\item Find the shape of an airplane wing which minimizes \href{https://en.wikipedia.org/wiki/Drag_(physics)}{drag}. Here the constraints could be the wing strength, or the wing dimensions.
	\item Find the shape of various mechanical structures, which can resist a given \href{https://en.wikipedia.org/wiki/Stress_(physics)}{stress} while having a minimal mass\texttt{/}volume.
	\item Given a known 3D object with a fixed radiation source inside, deduce the shape \& size of the source based on measurements done on part of the boundary of the object. A formulation of this \href{https://en.wikipedia.org/wiki/Inverse_problem}{inverse problem} using \href{https://en.wikipedia.org/wiki/Least-squares}{least squares} fit leads to a shape optimization problem.'' -- \href{https://en.wikipedia.org/wiki/Shape_optimization#Examples}{Wikipedia\texttt{/}shape optimization\texttt{/}examples}
\end{itemize}

\subsection{Techniques}
``Shape optimization problems are usually solved \href{https://en.wikipedia.org/wiki/Numerical_analysis}{numerically}, by using \href{https://en.wikipedia.org/wiki/Iterative_method}{iterative methods}. I.e., one starts with an initial guess for a shape, \& then gradually evolves it, until it morphs into the optimal shape.'' -- \href{https://en.wikipedia.org/wiki/Shape_optimization#Techniques}{Wikipedia\texttt{/}shape optimization\texttt{/}techniques}

\subsubsection{Keeping track of the shape}
\textsf{Fig. Example: Shape optimization as applied to building geometry. Example provided courtesy of \url{Formsolver.com}.}

``To solve a shape optimization problem, one needs to find a way to represent a shape in the \href{https://en.wikipedia.org/wiki/Computer_memory}{computer memory}, \& follow its evolution. Several approaches are usually used.

1 approach is to follow the boundary of the shape. For that, one can sample the shape boundary in a relatively dense \& uniform manner, i.e., to consider enough points to get a sufficiently accurate outline of the shape. Then, one can evolve the shape by gradually moving the boundary points. This is called the \textit{Lagrangian approach}.

Another approach is to consider a \href{https://en.wikipedia.org/wiki/Function_(mathematics)}{function} defined on a rectangular box around the shape, which is positive inside of the shape, zero on the boundary of the shape, \& negative outside of the shape. One can then evolve this function instead of the shape itself. One can consider a rectangular grid on the box \& sample the function at the grid points. As the shape evolves, the grid points do not change; only the function values at the grid points change. This approach, of using a fixed grid, is called the \textit{Eulerian approach}. The idea of using a function to represent the shape is at the basis of the \href{https://en.wikipedia.org/wiki/Level-set_method}{level set method}.

\textsf{Fig. Example: Optimization shape families resulting from differing goal parameters. Example provided courtesy of \url{Formsolver.com}}

A 3rd approach is to \fbox{think of the shape evolution as of a flow problem}. I.e., one can imagine that the shape is made of a plastic material gradually deforming s.t. any point inside or on the boundary of the shape can be always traced back to a point of the original shape in a 1-1 fashion. Mathematically, if $\Omega_0$ is the initial shape, \& $\Omega_t$ is the shape at time $t$, one considers the \href{https://en.wikipedia.org/wiki/Diffeomorphism}{diffeomorphisms} $f_t:\Omega_0\to\Omega_t$, for $t\le t\le t_0$. The idea is again that shapes are difficult entities to be dealt with directly, so manipulate them by means of a function.'' -- \href{https://en.wikipedia.org/wiki/Shape_optimization#Keeping_track_of_the_shape}{Wikipedia\texttt{/}shape optimization\texttt{/}techniques\texttt{/}keeping track of the shape}

\subsubsection{Iterative methods using shape gradients}
``Consider a smooth velocity field $V$ \& the family of transformations $T_s$ of the initial domain $\Omega_0$ under the velocity field $V$: $x(0) = x_0\in\Omega_0$, $x'(s) = V(x(s))$, $T_s(x_0) = x(s)$, $s\ge 0$, \& denote $\Omega_0\mapsto T_s(\Omega_0) = \Omega_s$. Then the G\^ateaux or shape derivative of $\mathcal{F}(\Omega)$ at $\Omega_0$ w.r.t. the shape is the limit of
\begin{align*}
	d\mathcal{F}(\Omega_0;V) = \lim_{s\to 0} \frac{\mathcal{F}(\Omega_s) - \mathcal{F}(\Omega_0)}{s}
\end{align*}
if this limit exists. If in addition the derivative is linear w.r.t. $V$, there is a unique element of $\nabla\mathcal{F}\in L^2(\partial\Omega)$ \& $d\mathcal{F}(\Omega_0;V) = \langle\nabla\mathcal{F},V\rangle_{\partial\Omega_0}$ where $\nabla\mathcal{F}$ is called the \textit{shape gradient}. This gives a natural idea of \href{https://en.wikipedia.org/wiki/Gradient_descent}{gradient descent}, where the boundary $\partial\Omega$ is evolved in the direction of negative shape gradient in order to reduce the value of the cost functional. Higher order derivatives can be similarly defined, leading to Newtonlike methods.

Typically, gradient descent is preferred, even if requires a large number of iterations, because, it can be hard to compute the 2nd-order derivative (i.e., the \href{https://en.wikipedia.org/wiki/Hessian_matrix}{Hessian}) of the objective functional $\mathcal{F}$.

If the shape optimization problem has constrains, i.e., the functional $\mathcal{G}$ is present, one has to find ways to convert the constrained problem into an unconstrained one. Sometimes ideas based on \href{https://en.wikipedia.org/wiki/Lagrange_multipliers}{Lagrange multipliers}, like the \href{https://en.wikipedia.org/wiki/Adjoint_state_method}{adjoint state method}, can work.'' -- \href{https://en.wikipedia.org/wiki/Shape_optimization#Iterative_methods_using_shape_gradients}{Wikipedia\texttt{/}shape optimization\texttt{/}techniques\texttt{/}iterative methods using shape gradients}

\subsubsection{Geometry parametrization}
``Shape optimization can be faced using standard optimization methods if a parametrization of the geometry is defined. Such parametrization is very important in CAE field where goal functions are usually complex functions evaluated using numerical models (CFD, FEA, $\ldots$). A convenient approach, suitable for a wide class of problems, consists in the paramtrization of the CAD model coupled with a full automation of all the process required for function evaluation (meshing, solving \& result processing). \textit{Mesh morphing} is a valid choice for complex problems that resolves typical issues associated with \textit{re-meshing} such as discontinuities in the computed objective \& constraint functions. In this case the parametrization is defined after the meshing stage acting directly on the numerical model used for calculation that is changed using mesh updating methods. There are several algorithms available for mesh morphing (\textit{deforming volumes, pseudosolids}, \href{https://en.wikipedia.org/wiki/Radial_basis_function}{radical basis functions}). The selection of the parametrization approach depends mainly on the size of the problem: the CAD approach is preferred for small-to-medium sized models whilst the mesh morphing approach is the best (\& sometimes the only feasible one) for large \& very large models. The multi-objective Pareto optimization (NSGA II) could be utilized as a powerful approach for shape optimization. In this regard, the Pareto optimization approach displays useful advantages in design method such as the effect of area constraint that other multi-objective optimization cannot declare it. The approach of using a penalty function is an effective technique which could be used in the 1st stage of optimization. In this method the constrained shape design problem is adapted to an unconstrained problem with utilizing the constraints in the objective function as a penalty factor. Most of the time penalty factor is dependent to the amount of constraint variation rather than constrain number. The GA real-coded technique is applied in the present optimization problem. Therefore, the calculations are based on real value of variables.'' -- \href{https://en.wikipedia.org/wiki/Shape_optimization#Geometry_parametrization}{Wikipedia\texttt{/}shape optimization\texttt{/}techniques\texttt{/}geometry parametrization}

%------------------------------------------------------------------------------%

\chapter{\href{formsolver.com}{Formsolver.com}}

%------------------------------------------------------------------------------%

\chapter{Optimal Control}

\section{Introduction}
``The mathematical optimization of process governed by PDEs has seen considerable progress in the past decade. Ever faster computational facilities \& newly developed numerical techniques have opened the door to important practical applications in fields e.g. fluid flow, microelectronics\footnote{\textbf{microelectronics} [n] [uncountable] the design, production \& use of very small electronic circuits.}, crystal\footnote{\textbf{crystal} [n] \textbf{1.} [countable] a small piece of a substance with many even sides, that is formed naturally when the substance becomes solid; in chemistry, a \textbf{crystal} is any solid that has its atoms, ions or molecules arranged in an ordered, symmetrical way; \textbf{2.} [uncountable] a clear mineral, e.g. quartz, used in making decorative objects.} growth, vascular\footnote{\textbf{vascular} [a] [usually before noun] (\textit{medical}) connected with or containing veins.} surgery\footnote{\textbf{surgery} [n] \textbf{1.} [uncountable, countable] medical treatment of injuries or diseases that involves cutting open a person's body, sewing up wounds, etc.; \textbf{2.} [countable] (\textit{British English}) a place where a doctor sees patients; \textbf{3.} [countable] (\textit{British English}) a time during which a doctor, an MP or another professional person is available to see people.}, \& cardiac\footnote{\textbf{cardiac} [a] [only before noun] (\textit{medical}) connected with the heart or heart disease; if somebody has a \textbf{cardiac arrest}, their heart suddenly stops temporarily or permanently.} medicine, to name just a few. As a consequence, the communities of numerical analysts \& optimizers have taken a growing interest in applying their methods to optimal control problems involving PDEs $\ldots$'' [$\ldots$] ``$\ldots$ the comprehensive text by J.-L. Lions \cite{Lions1971} covers much of the theory of linear equations \& convex cost functionals.'' -- \cite[Preface to the German edition, p. xiii]{Troltzsch2010}

\cite{Troltzsch2010} focuses on basic concepts \& notions e.g.:
\begin{itemize}
	\item Existence theory for linear \& semilinear PDEs
	\item Existence of optimal controls
	\item Necessary optimality conditions \& adjoint equations
	\item 2nd-order sufficient optimality conditions
	\item Foundation of numerical methods
\end{itemize}

\begin{question}
	What is optimal control?
\end{question}
``The mathematical theory of optimal control has in the past few decades rapidly developed into an important \& separate field of applied mathematics. 1 area of application of this theory lies in aviation\footnote{\textbf{aviation} [n] [uncountable] the activity of designing, building \& flying aircraft.} \& space technology: aspects of optimization come into play whenever the motion of an aircraft or a space vessel\footnote{\textbf{vessel} [n] \textbf{1.} a tube that carries blood through the body of a person or an animal, or liquid through the parts of a plant; \textbf{2.} (\textit{formal}) a large ship or boat; \textbf{3.} (\textit{formal}) a container used for holding liquids, e.g. a bowl or cup.} (which can be modeled by ODEs) has to follow a trajectory\footnote{\textbf{trajectory} [n] (plural \textbf{trajectories}) (\textit{specialist}) \textbf{1.} the curved part of something that has been fired, hit or thrown into the air; \textbf{2.} the way in which a person, an event or a process develops over a period of time, often leading to a particular result.} that is ``optimal'' in a sense to be specified.'' -- \cite[Sect. 1.1: \textit{What is optimal control?}, p. 1]{Troltzsch2010}

All the essential features of an \textit{optimal control problem}:
\begin{itemize}
	\item a \textit{cost functional} to be minimized,
	\item an IVP for an ODE in order to determine the \textit{state} $y$,
	\item a \textit{control function} $u$, \&
	\item various constraints that have to be obeyed.
\end{itemize}
``The control $u$ may be freely chosen within the given constraints, while the state is uniquely determined by the differential equation \& the initial conditions. We have to choose $u$ in such a way that the cost function is minimized. Such controls are called \textit{optimal}.'' [$\ldots$] ``The optimal control of ODEs is of interest not only for aviation \& space technology. In fact, it is also important in fields e.g. robotics\footnote{\textbf{robotics} [n] [uncountable] the science of designing \& operating robots.}, movement sequences in sports, \& the control of chemical processes \& power plants, to name just a few of the various applications. In many cases, however, the processes to be optimized can no longer be adequately modeled by ODEs; instead, PDEs have to be employed for their description. E.g., heat conduction\footnote{\textbf{conduction} [n] [uncountable] (\textit{physics}) the process by which heat or electricity passes along or through a material.}, diffusion\footnote{\textbf{diffusion} [n] [uncountable] \textbf{1.} the spreading of something more widely; \textbf{2.} the mixing of substances by the natural movement of their particles; \textbf{3.} the spreading of elements of culture from 1 region or group to another.}, electromagnetic\footnote{\textbf{electromagnetic} [a] (\textit{physics}) in which the electrical \& magnetic properties of something are related.} waves, fluid flows, freezing processes, \& many other physical phenomenon\footnote{\textbf{phenomenon} [n] (plural \textbf{phenomena} a fact or an event in nature or society, especially one that is not fully understood.)} can be modeled by PDEs.

In these fields, there are numerous interesting problems in which a given cost functional has to be minimized subject to a differential equation \& certain constraints being satisfied. The difference from the above problem ``merely'' consists of the fact that a PDE has to be dealt with in place of an ordinary one.'' -- \cite[pp. 2--3]{Troltzsch2010}

\cite{Troltzsch2010} discusses, ``through examples in the form of mathematically simplified case studies, the optimal control of heating processes, 2-phase problems, \& fluid flows''. \cite{Troltzsch2010} focuses ``on linear \& semilinear elliptic \& parabolic PDEs, since a satisfactory regularity theory is available for the solutions to such equations. This is not the case for hyperbolic equations. Also, the treatment of quasilinear PDEs is considerably more difficult, \& the theory of their optimal control is still an open field in many respects.'' [$\ldots$] ``$\ldots$ the Hilbert space setting suffices as a functional analytic framework in the case of linear-quadratic theory.'' -- \cite[p. 3]{Troltzsch2010}

\subsection{Examples of Convex Problems}

\subsubsection{Optimal boundary heating}
See \cite[Subsect. 1.2.1, pp. 3--5]{Troltzsch2010}.

\begin{example}[Optimal boundary heating]
	Consider a body heated or cooled which occupies the spatial domain $\Omega\subset\mathbb{R}^3$. Apply to its boundary $\Gamma$ a \emph{heat source} $u$ (the \emph{control}), which is constant in time but depends on the location ${\bf x}$ on the boundary, i.e., $u = u({\bf x})$. Aim: choose the control in such a way that the corresponding \emph{temperature distribution} $y = y({\bf x})$ in $\Omega$ (the \emph{state}) is the best possible approximation to a desired stationary temperature distribution $y_\Omega = y_\Omega({\bf x})$:
	\begin{align*}
		\min J(y,u)\coloneqq\frac{1}{2}\int_\Omega |y({\bf x}) - y_\Omega({\bf x})|^2\,{\rm d}{\bf x} + \frac{\lambda}{2}\int_\Gamma |u({\bf x})|^2\,{\rm d}s({\bf x}),
	\end{align*}
	subject to the \emph{state equation}:
	\begin{equation*}
		\left\{\begin{split}
			-\Delta y &= 0,&&\mbox{ in }\Omega,\\
			\partial_{\bf n}y &= \alpha(u - y),&&\mbox{ on }\Gamma,
		\end{split}\right.
	\end{equation*}
	and the \emph{pointwise control constraints} $u_a({\bf x})\le u({\bf x})\le u_b({\bf x})$ on $\Gamma$. ``Such pointwise bounds for the control are quite natural, since the available capacities for heating or cooling are usually restricted. The constant $\lambda\ge 0$ can be viewed as a measure of the energy costs needed to implement the control $u$. From the mathematical viewpoint, this term also serves as a \emph{regularization parameter}; it has the effect that possible optimal controls show improved regularity properties.'' [$\ldots$] ``The function $\alpha$ represents the \emph{heat transmission coefficient} from $\Omega$ to the surrounding medium. The functional $J$ to be minimized is called the \emph{cost functional}. The factor $\frac{1}{2}$ appearing in it has no influence on the solution of the problem. It is introduced just for the sake of convenience: it will later cancel out a factor 2 arising from differentiation. We seek an optimal control $u = u({\bf x})$ together with the associated state $y = y({\bf x})$. The minus sign in front of the Laplacian $\Delta$ appears to be unmotivated at 1st glance. It is introduced because $\Delta$ is not a \emph{coercive operator}, while $-\Delta$ is.'' -- \cite[p. 4]{Troltzsch2010}
\end{example}
``Observe that in the above problem the cost functional is quadratic, the state is governed by a linear elliptic PDE, \& the control acts on the boundary of the domain.'': thus have a \textit{linear-quadratic elliptic boundary control problem}.

\begin{remark}[Notations used in \cite{Troltzsch2010}]
	Denote the element of surface area by $ds$ \& the outward unit normal to $\Gamma$ at ${\bf x}\in\Gamma$ by $\nu({\bf x})$\footnote{NQBH: I prefer to use ${\bf n}({\bf x})$, with ``n'' stands for ``normal'', naturally \& obviously.}.
\end{remark}

\begin{remark}
	``The problem is strongly simplified. Indeed, in a realistic model Laplace's equation $\Delta y = 0$ has to be replaced by the stationary heat conduction equation $\nabla\cdot(a\nabla y) = 0$, where the coefficient $a$ can depend on ${\bf x}$ or even on $y$. If $a = a(y)$ or $a = a({\b f x},y)$, then the PDE is quasilinear. In addition, it will in many cases be more natural to describe the process by a time-dependent PDE.'' -- \cite[p. 4]{Troltzsch2010}
\end{remark}

\begin{example}[Optimal heat source]
	Similarly, the control can act as a \emph{heat source in the domain} $\Omega$. Problems of this kind arise if the body $\Omega$ is heated by electromagnetic induction or by microwaves. Assuming at 1st that the boundary temperature vanishes, we obtain the following problem:
	\begin{align*}
		\min J(y,u)\coloneqq\frac{1}{2}\int_\Omega |y({\bf x}) - y_\Omega({\bf x})|^2\,{\rm d}{\bf x} + \frac{\lambda}{2}\int_\Omega |u({\bf x})|^2\,{\rm d}{\bf x},
	\end{align*}
	subject to
	\begin{equation*}
		\left\{\begin{split}
			-\Delta y &= \beta u,&&\mbox{ in }\Omega,\\
			y &= 0,&&\mbox{ on }\Gamma,
		\end{split}\right.
	\end{equation*}
	and $u_a({\bf x})\le u({\bf x})\le u_b({\bf x})$ in $\Omega$. Here, the coefficient $\beta = \beta({\bf x})$ is prescribed. Observe that by the special choice $\beta = \chi_{\Omega_{\rm c}}$ (where $\chi_E$ denotes the characteristic function of a set $E$), it can be achieved that $u$ acts only in a subdomain $\Omega_{\rm c}\subset\Omega$. This problem is a \emph{linear-quadratic elliptic control problem with distributed control}. It can be more realistic to prescribe an exterior temperature $y_a$ rather than assume that the boundary temperature vanishes. Then a better model is given by the state equation
	\begin{equation*}
		\left\{\begin{split}
			-\Delta y &= \beta u,&&\mbox{ in }\Omega,\\
			\partial_{\bf n}y &= \alpha(y_a - y),&&\mbox{ on }\Gamma.
		\end{split}\right.
	\end{equation*}
\end{example}

\subsubsection{Optimal nonstationary boundary control}
See \cite[pp. 5--6]{Troltzsch2010}. ``Let $\Omega\subset\mathbb{R}^3$ represent a potato that is to be roasted over a fire for some period of time $T > 0$.'' Denote its temperature by $y = y(t,{\bf x})$, with $(t,x)\in[0,T]\times\Omega$. ``Initially, the potato has temperature $y_0 = y_0({\bf x})$, \& we want to serve it at a pleasant palatable\footnote{\textbf{palatable} [a] \textbf{1.} (of food or drink) having a pleasant or acceptable taste; \textbf{2.} \textbf{palatable (to somebody)} pleasant or acceptable to somebody, \textsc{opposite}: \textbf{unpalatable}.} temperature $y_\Omega$ at the final time $T$.'' Write $Q\coloneqq(0,T)\times\Omega$, $\Sigma\coloneqq(0,T)\times\Gamma$. Then problem reads as follows:
\begin{align*}
	\min J(y,u)\coloneqq\frac{1}{2}\int_\Omega |y(T,{\bf x}) - y_\Omega({\bf x})|^2\,{\rm d}{\bf x} + \frac{\lambda}{2}\int_0^T\int_\Gamma |u(t,{\bf x})|^2\,{\rm d}\Gamma\,{\rm d}t,
\end{align*}
subject to
\begin{equation*}
	\left\{\begin{split}
		y_t - \Delta y &= 0,&&\mbox{ in } Q,\\
		\partial_{\bf n}y &= \alpha(u - y),&&\mbox{ on }\Sigma,\\
		y(0,{\bf x}) &= y_0({\bf x}),&&\mbox{ in }\Omega,
	\end{split}\right.
\end{equation*}
\& $u_a(t,{\bf x})\le u(t,{\bf x})\le u_b(t,{\bf x})$ on $\Sigma$. By continued turning of the spit\footnote{\textbf{spit} [n] \textit{in}\texttt{/}\textit{from mouth} \textbf{1.} [uncountable] the liquid produced in your mouth, \textsc{synonym}: \textbf{saliva}; \textbf{2.} [countable, usually singular] the act of spitting liquid or food out of your mouth; \textit{piece of land} \textbf{3.} [countable] a long, thin piece of land that sticks out into the sea, a lake, etc.; \textit{for cooking meat} \textbf{4.} [countable] a long, thin, straight piece of metal that you put through meat to hold \& turn it while you cook it over a fire.}, we produce $u(t,{\bf x})$. The heating process has to be described by the \textit{nonstationary heat equation}, which is a parabolic differential equation: thus have to deal with a \textit{linear-quadratic parabolic boundary control problem}.

\subsubsection{Optimal vibrations}
``Suppose that a group of pedestrians crosses a bridge, trying to excite\footnote{\textbf{excite} [v] \textbf{1.} to make somebody feel a particular emotion or react in a particular way, \textsc{synonym}: \textbf{arouse}; \textbf{2.} \textbf{excite somebody} to make somebody feel very pleased, interested or enthusiastic, especially about something that is going to happen; \textbf{3.} \textbf{excite somebody\texttt{/}something} to make somebody\texttt{/}something nervous, upset or active \& unable to relax; \textbf{4.} \textbf{excite something} to produce a state of increased energy or activity in a physical or biological system, \textsc{synonym}: \textbf{stimulate}; \textbf{5.} \textbf{excite something} (\textit{physics}) to bring something to a state of higher energy.} oscillations\footnote{\textbf{oscillation} [n] \textbf{1.} [countable, uncountable] \textbf{oscillation (of something)} a regular movement between 1 position \& another; \textbf{2.} [countable] \textbf{oscillation (between A \& B)} a repeated change between different states, ideas, etc.; \textbf{3.} [countable] (\textit{specialist} regular variation in size, strength or position around a central point or value, especially of an electrical current or electric field.)} in it. This can be modeled (strongly abstracted) as follows: let $\Omega\subset\mathbb{R}^2$ denote the domain of the bridge, $y = y(t,{\bf x})$ its \textit{transversal\footnote{\textbf{transversal} [n] a line that intersects a system of lines.} displacement\footnote{\textbf{displacement} [n] \textbf{1.} [uncountable] the act of displacing somebody\texttt{/}something; the process of being displaced; \textbf{2.} [uncountable, singular] \textbf{displacement (of something)} (\textit{physics}) the distance between the final \& initial ($=$ 1st) positions of an object which has moved.}}, $u = u(t,{\bf x})$ the \textit{force density} acting in the vertical direction, \& $y_{\rm d} = y_{\rm d}(t,{\bf x})$ a \textit{desired evolution of the transversal vibrations\footnote{\textbf{vibration} [n] [countable, uncountable] \textbf{1.} \textbf{vibration (of something)} a continuous shaking movement; \textbf{2.} \textbf{vibration (of something)} (\textit{physics}) oscillation in a substance about its equilibrium state.}}. We then obtain the optimal control problem:
\begin{align*}
	\min J(y,u)\coloneqq\frac{1}{2}\int_0^T\int_\Omega |y(t,{\bf x}) - y_{\rm d}(t,{\bf x})|^2\,{\rm d}{\bf x}\,{\rm d}t + \frac{\lambda}{2}\int_0^T\int_\Omega |u(t,{\bf x})|^2\,{\rm d}{\bf x}\,{\rm d}t,
\end{align*}
subject to
\begin{equation*}
	\left\{\begin{split}
		y_{tt} - \Delta y &= u,&&\mbox{ in } Q,\\
		y(0) &= y_0,&&\mbox{ in }\Omega,\\
		y_t(0) &= y_1,&&\mbox{ in }\Omega,\\
		y &= 0,&&\mbox{ on }\Sigma,
	\end{split}\right.
\end{equation*}
and $u_a(t,{\bf x})\le u(t,{\bf x})\le u_b(t,{\bf x})$ in $Q$. This is a \textit{linear-quadratic hyperbolic control problem with distributed control}.'' [$\ldots$] ``Interesting control problems for oscillating elastic networks have been treated by Lagnese et al. \textbf{[LLS94]}. An elementary introduction to the controllability of oscillations can be found in \textbf{[Kra95]}.

In the linear-quadratic case, the theory of hyperbolic problems has many similarities to the parabolic theory studied in \cite{Troltzsch2010}. However, the treatment of semilinear hyperbolic problems is much more difficult, since the smoothing properties of the associated solution operators are weaker. As a consequence, many of the techniques presented in \cite{Troltzsch2010} fail in the hyperbolic case.'' -- \cite[pp. 6--7]{Troltzsch2010}

\subsection{Examples of Nonconvex Problems}
``However, linear models do not suffice for many real-world phenomena. Instead, one often needs quasilinear or, much simpler, semilinear equations. Recall that a 2nd-order equation is called \textit{semilinear} if the main parts (i.e., the expressions involving highest-order derivatives) of the differential operators considered in the domain \& on the boundary are linear w.r.t. the desired solution. For such equations, the theory of optimal control is well developed.

\fbox{Optimal control problems with semilinear state equations are, as a rule, nonconvex, even if the cost functional is convex.} ``Associated optimal control problems can be obtained by prescribing a cost functional \& suitable constraints.'' -- \cite[p. 7]{Troltzsch2010}

\subsubsection{Problems involving semilinear elliptic equations}

\begin{example}[Heating with radiation boundary condition]
	If the heat radiation of the heated body is taken into account, then we obtain a problem with a nonlinear Stefan--Boltzmann boundary condition. If this case, the control $u$ is given by the temperature of the surrounding medium:
	\begin{equation*}
		\left\{\begin{split}
			-\Delta y &= 0,&&\mbox{ in }\Omega,\\
			\partial_{\bf n}y &= \alpha(u^4 - y^4),&&\mbox{ on }\Gamma.
		\end{split}\right.
	\end{equation*}
	The nonlinearity $y^4$ occurs in the boundary condition, while the heat conduction equation itself is linear.
\end{example}

\begin{example}[Simplified superconductivity]
	
\end{example}

\begin{example}[Control of stationary flows]
	
\end{example}

\subsubsection{Problems involving semilinear parabolic equations}

\section*{Quick notes}
\textit{Primal-dual active set strategies}, whose the exposition now leads to the systems of linear equations to be solved.

%------------------------------------------------------------------------------%

\chapter{Shape Optimization}

\section{Introduction}
``Shape Optimization was introduced around 1970 by Jean C\'ea \cite{Cea_Gioan_Michel1973}, who understood, after several engineering studies [127, 12, 35, 110, 102, 83, 84, 7],\footnote{(see refs. in \cite{Moubachir_Zolesio2006})}, the future issues in the context of optimization problems. At that time, he proposed a list of open problems at the French National Colloquium in Numerical Analysis. These new problems were formulated in terms of minimization of functionals (referred as \textit{open loop control} or \textit{passive control}) governed by partial differential BVPs where the control variable was the geometry of a given boundary part [103, 76]. From the beginning, the terminology \textit{shape optimization} was not connected to the structural mechanical sciences in which elasticity \& optimization of the compliance played a central role. Furthermore, these research studies were mainly addressed in the context of the numerical analysis of the FEMs.

At the same time, there was some independent close results concerning fluid mechanics by young researchers e.g. O. Pironneau [123, 124, 78], Ph. Morice [107] \& also several approaches related to perturbation theory by P.R. Garabedian [74, 75] \& D.D Joseph [91, 92].

Very soon, it appeared that the shape control of BVPs was at the crossroads of several disciplines such as PDE analysis, non-autonomous semi-group theory, numerical approximation (including FEMs), control \& optimization theory, geometry \& even physics. Indeed several classical modeling in both structural \& fluid mechanics (among other fields) needed to be extended. An illustrative example concerns a very \textit{popular} problem in the 80's concerning the thickness optimization of a plate modeled by the classical Kirchoff biharmonic equation. This kind of solid model is based on the assumption that the thickness undergoes only small variations. Therefore, many pioneering works were violating the validity of this assumption, leading to strange results, e.g., the work presented in the Iowa NATO Study [85] stating the existence of optimal beams having \textit{zero cross section} values.

In the \textit{branch} which followed the passive control approach, we shall mention the work of G. Chavent [32, 34] based on the theory of distributed system control introduced by J.-L. Lions \cite{Lions1971}. Those results did not address optimization problems related to the domain but instead related to the coefficients inside the PDE. At that time, it was hoped that the solution of elliptic problems would be continuous w.r.t. the weak convergence of the coefficients. It appeared that this property was not achieved by this class of problem\footnote{Indeed, in his thesis [33], G. Chavent referred to such a result to appear in a work by F. Murat [113]. That paper [111] appeared but as a counterexample to the expected continuity property. He showed on a 1D simple example that with weak \textit{oscillating} convergence of the coefficients, the associated solution was converging to another problem in which the new coefficients were related to the limit of the \textit{inverse} coefficients associated to the original problem [112, 114].} At that point a main \textit{bifurcation} arose with the homogenization approach [10] which up to some point was considered as a part of the \textit{Optimal Design} theory.

The mathematical analysis of shape optimization problems began with the correct definition of derivatives of functionals \& functions w.r.t. the domain, together with the choice of tangential space to the family of shapes. Following the very powerful theory developed by J. Ne\v{c}as [117], the role of bilipschitzian mapping was emphasized for Sobolev spaces defined in moving domains based on the Identity perturbation method [115, 106, 134]. Concerning the large domain deformation viewpoint the previous approach led to the incremental domain evolution methods [143].

After 1975, the 2nd author introduced [145] an asymptotic analysis for domain evolution using classical geometrical flows which are intrinsic tools for manifolds evolutions \& gave existence results for the so-called \textit{shape differential equation} (see also [79]). At that period, applications focused more on sensitivity analysis problems than on asymptotic analysis of domains evolution. In 1972, A.M. Micheletti introduced in parallel [105, 104] a metric based on the Identity perturbation method thanks to the use of differentiable mappings, in order to study eigenvalues perturbation problems. The associated topology was extended by M. Delfour et. al [52] \& turns out to be the same as the one induced by the continuity along flow field deformations [147].

The systematic use of flow mapping \& intrinsic geometry through the fundamental role of the oriented distance function [47, 50] led to the revised analysis of the elastic shell theory [48, 49, 25, 26, 27, 28], of the boundary layer theory [3] or of the manifold derivation tools [53].

The use of both Bounded Variation (BV) analysis \& the notion of Cacciapoli sets led to the 1st compactness method for domain sequences \& several extensions to more regular boundaries were done through the use of different concepts such as \textit{fractal boundaries, density parameter} [23, 20, 21, 19] or \textit{Sobolev domains} [50].

At that point, an other important \textit{bifurcation point} in that theory occurred with the relaxation theory \& the Special Bounded Variation (SBV) analysis which was particularly well adapted for image segmentation problem [6]. At the opposite, the capacity constraint for Dirichlet boundary conditions led to a fine analysis initiated in [18] \& is still going on for cracks analysis.

The method of large evolution based on the flow mapping (known from 1980 as the \textit{speed method} [150]) turns to be the natural setting for weak evolution of geometry allowing topological changes through the convection of either characteristic functions or oriented distance functions.'' -- \cite[Chap. 1, pp. 1--3]{Moubachir_Zolesio2006}

\subsection{Classical \& moving shape analysis}
``The classical shape analysis investigates the effects of perturbations of the geometry in terms of continuity, differentiability \& optimization of quantities related to the state of a system defined in that geometry. In this case, the geometry is usually perturbed thanks to a map involving a scalar parameter usually referred to as a fictitious time. On the contrary, the moving shape analysis deals with systems that are intrinsically defined on a moving geometry. Hence, we shall deal with sensitivity analysis w.r.t. a continuous famil of shapes over a given time period. In this context, if we consider the geometry in a space-time configuration, the moving shape analysis may also be referred to as a \textit{non-cylindrical shape analysis}\footnote{The notion of tube (non-cylindrical evolution domains) was also independently introduced by J.P. Aubin via the concept of \textit{abstract mutations} [8].}.

A 1st issue in this analysis is to model the evolution of the geometry. This is a common topic with the classical shape analysis. There exists many ways to build families of geometries. E.g., a domain can be made variable by considering its image by a family of diffeomorphisms parametrized by the time parameter as it happens frequently in mechanics for the evolution of continuous media. This way of defining the motion of domains avoids a priori the modification of the underlying topology. This change of topology can be allowed by using the characteristic function of families of sets or the level set of a space-time scalar function.'' Refer to \cite{Delfour_Zolesio2001, Delfour_Zolesio2011} for a complete review on this topic. ``In Chap. 2, we shall deal with the particular problem of defining in a weak manner the convection of a characteristic function in the context of the \textit{speed method} developed in Zol\'esio's PhD thesis [147].

In numbers of applications, we shall consider a state variable associated to a system which is a solution of a PDE defined inside the moving domain over a given time period. Hence, we need to analyze the solvability of this non-cylindrical PDE system before going further. Here, again this topic has been already studied since it enters the classical shape analysis problem while introducing a perturbed state defined in the moving domain parametrized by the fictitious time parameter. Furthermore, this solvability analysis has been performed in numbers of mathematical problems involving moving domains.'' Refer to [135, 51] for some particular results in the context of the classical shape analysis. Also refer to the extensive literature concerning the analysis of PDE systems defined in moving domains, e.g., [96, 126, 132, 62, 130, 88, 128, 70, 100, 11].

``Contrary to the last topic, very few references exist for the sensitivity analysis w.r.t. the perturbation of the evolution of the moving geometry. Early studies have been conducted in [90, 151, 158, 141, 120, 43, 142, 2] for specific hyperbolic \& parabolic linear problems. An important step was performed in [154, 155] where Zol\'esio established the derivative of integrals over a moving domain w.r.t. its associated Eulerian velocity. These results were applied in order to study variational principles for an elastic solid under large displacements \& the incompressible Euler equation. This work was generalized in [58, 59].'' -- \cite[Chap. 1, Sect. 1.1, pp. 3--4]{Moubachir_Zolesio2006}

\subsection{Fluid-Structure Interaction Problems}
``A general fluid--solid model consists of an elastic solid either surrounded by a fluid (aircrafts, automobiles, bridge decks, $\ldots$) or surrounding a fluid flow (pipelines, arteries, reservoir tanks, $\ldots$). Here the motion of the interface between the fluid \& the solid is part of the unknown of the coupled system. It is a free boundary problem that can be solved by imposing continuity properties through the moving interface (e.g., the kinematic continuity of the velocities \& the kinetic continuity of the normal stresses). This model has been intensively studied in the last 2 decades on the level of its mathematical solvability [87, 54, 82, 39, 80, 131, 15, 9, 138, 41], its numerical approximation [89, 55, 119, 118, 122, 95, 67], its stability [73, 38, 64, 65] \& more recently on its controllability [66, 109]. In this lecture note, we will restrict ourselves to viscous Newtonian incompressible fluid flows described by the NSEs in space dimension 2 or 3. The case of a compressible Newtonian fluid can be incorporated in the present framework with the price of a heavier mathematical analysis (solvability, non-differentiability around shocks $\ldots$).''

\textbf{Goal.} ``To solve inverse or control problems based on the previous general fluid-solid model. As an example, we think to decrease the drag of a car inside the atmospheric air flow by producing specific vibrations on its body using smart materials such as piezoelectrical layers. In this example, the control variable can be chosen as the electrical energy input evolution inside the piezoelectrical device \& the objective is to decrease the drag which is a function of the coupled fluid-structure state (the air \& the body of the car) \& this state depends on the control variable. In order to build a control law for the electrical input, we need to characterize the relationship between the drag function \& the control variable on the level of its computation \& its variations.

As an other example, we can think of the problem of aeroelastic stability of structures. Both authors have been dealing with such a problem in the context of the stability analysis against wind loads of bridge decks. In [108], it has been suggested that such a problem can be set as the inverse problem consisting in recovering the smallest upstream wind speed that leads to the worst bridge deck vibrations. In this example, the decision variable can be chosen as the upstream wind speed \& the objective is to increase a functional based on the vibration amplitude history of the bridge deck during a given characteristic time period which is a function of the coupled fluid-structure state (the wind flow \& the bridge deck) which is also a function of the decision variable. Again, in order to recover the wind speed history, we need to characterize the relationship between the objective functional \& the decision variable on the level of its computation \& its variations.

In order to characterize the sensitivity of the objective functional w.r.t. the control variable, it is obvious that we need to characterize the sensitivity of the coupled fluid-structure state w.r.t. the control variable. Here we recall that the coupled fluid-structure state is the solution of a system of PDEs that are coupled through continuity relations defined on the moving interface (the fluid-structure interface). The key point towards this sensitivity analysis is to investigate the sensitivity of the fluid state, which is an Eulerian quantity, w.r.t. the motion of the solid, which is a Lagrangian quantity. This task falls inside the moving shape analysis framework described earlier. Indeed the fluid state is the solution of system of nonlinear PDEs defined in a moving domain. The boundary of this moving domain is the solid wall. Then using the tools developed in [59], it has been possible to perform in [58] the moving shape sensitivity analysis in the case of a Newtonian incompressible fluid inside a moving domain driven by the non-cylindrical NSEs.

All the previous results use a parametrization of the moving domain based on the Lagrangian flow o a given velocity field. Hence, the design variable is the Eulerian velocity of the moving domain, allowing topology changes while using the associated level set formulation. In [13, 14], the author used a non-cylindrical identity perturbation technique. It consists in perturbating the space-time identity operator by a family of diffeomorphism. Then, this family is chosen as the design parameter. It is a Lagrangian description of the moving geometry, which a priori does not allow topology changes but which leads to simpler sensitivity analysis results which are are still comparable with the one obtained by the non-cylindrical \textit{speed method}. In [57], the authors came back to the dynamical shape control of the Navier-Stokes \& recovered the results obtained in [58] using the Min-Max principle allowing to avoid the state differentiation step w.r.t. the velocity of the domain.

Now, we come back to the original problem consisting in the sensitivity analysis of the coupled fluid-structure state w.r.t. the control variable. Using the chain rule, the derivative of the coupled state w.r.t. the control variable involves the partial derivative of the fluid state w.r.t. the motion of the fluid-structure interface already characterized in [58, 57]. Hence, again using a Lagrangian penalization technique, already used \& justified in [45, 46], it has been possible to perform in [109] the sensitivity analysis of a simple fluid-structure interaction problem involving a rigid solid within an incompressible flow of a Newtonian fluid w.r.t. the upstream velocity field. As already mentioned, this simple model is particularly suited for bridge deck aeroelastic stability analysis [121].'' -- \cite[Chap. 1, Sect. 1.2, pp. 4--6]{Moubachir_Zolesio2006}
\begin{itemize}
	\item ``\cite[Chap. 2]{Moubachir_Zolesio2006} furnishes a simple illustration to some of the moving shape analysis results reported in the core of the lecture note. We deal with a simple inverse problem arising in phase change problems consisting in recovering the moving interface at the isothermal interface between a solid \& liquid phase from measurements of the temperature on a insultated fixed part of the solid boundary. We use a least-square approach \& we show how to compute the gradient of the least-square functional w.r.t. the velocity of the moving interface. It involves an adjoint state problem together with an adjoint transverse state, which is the novelty of the moving shape analysis compared to the classical one.
	\item In \cite[Chap. 3]{Moubachir_Zolesio2006}, we consider the weak Eulerian evolution of domains through the convection, generated by a non-smooth vector field ${\bf V}$, of measurable sets. The introduction of transverse variations enables the derivation of functionals associated to evolution tubes. We also introduce Eulerian variational formulations for the minimal curve problem. These formulations involve a geometrical adjoint state $\lambda$ which is backward in time \& is obtained thanks to the use of the so-called \textit{transverse field} ${\bf Z}$.
	\item In \cite[Chap. 4]{Moubachir_Zolesio2006}, we recall the concept of shape differential equation developed in [145, 147]. Here, we present a simplified version \& some applications in 2D which enable us to reach the time asymptotic result. Furthermore, we introduce the associated level set formulation whose speed vector version was already contained in [149].
	\item In \cite[Chap. 5]{Moubachir_Zolesio2006}, we deal with a challenging problem in fluid mechanics which consists in the control of a Newtonian fluid flow thanks to the velocity evolution law of a moving wall. Here, the optimal control problem has to be understood as the open loop version, i.e., it consists in minimizing a given objective functional w.r.t. the velocity of the moving wall. This study is performed within the non-cylindrical Eulerian moving shape analysis described in Chaps. 2--3. We focus on the use of a Lagrangian penalization formulation in order to avoid the fluid state differentiation step.
	\item In \cite[Chap. 6]{Moubachir_Zolesio2006}, we introduce the Lagrangian moving shape analysis framework. It differs from the Eulerian one form the fact that the design variable is the diffeomorphism that parametrizes the moving geometry. The sensitivity analysis is simpler since it does not involve the transverse velocity field. We apply these tools in order to deal with the control of a Newtonian fluid flow thanks to the displacement evolution law of a moving wall.
	\item \cite[Chap. 7]{Moubachir_Zolesio2006} moves to inverse problems related to fluid-structure interaction systems. Here, we consider a 2D elastic solid with rigid displacements inside the incompressible flow of a viscous Newtonian fluid. We try to recover informations about the inflow velocity field from the partial measurements of the coupled fluid-structure state. We use a least-square approach together with a Lagrangian penalization technique. We derive the structure of the gradient w.r.t. the inflow velocity field of a given cost function. Using the Min-Max principle, the cost function gradient reduces to the derivative of the Lagrangian w.r.t. the inflow velocity at the saddle point. This saddle point is solution of 1st order optimality conditions. We use non-cylindrical Eulerian derivatives to compute the partial derivative of the Lagrangian functional w.r.t. the solid state variables, involved in the optimality system.
	\item In \cite[Chap. 8]{Moubachir_Zolesio2006} we extend the results of Chap. 7, to the case of an elastic solid under large displacements inside an incompressible fluid flow. The main difference with the previous case is the use of a non-cylindrical Lagrangian shape analysis for establishing the KKT system. It forms the adjoint counterpart of the sensitivity analysis conducted in [66].'' -- \cite[Chap. 1, Sect. 1.3, pp. 6--7]{Moubachir_Zolesio2006}
\end{itemize}
``$\ldots$ we shall describe the different steps encountered while designing a complex fluid-structure interaction system. Indeed, let us consider a mechanical system that consists of a solid \& a fluid interacting with each other. We would like to increase the performances of this system. These performances have to be quantitatively translated inside a cost function that we have to optimize w.r.t. some parameters that we will call the control variables. In the sequel, we will describe different control situations:
\begin{enumerate}
	\item \textit{Control of a fluid flow around a fixed body}: it consists in trying to modify the fluid flow pattern around a fixed body using a boundary control which can act e.g. by blowing or suctioning the fluid at some part of the solid boundary. The control law will be designed in order to match some efficiency goals using the minimization of a cost functional.
	\item \textit{Shape design of a fixed solid inside a fluid flow}: in this case, the control is the shape of the body. We would like to find the best shape satisfying some geometrical constraints that will optimize some cost functionals. This problem is somewhat classical in the aeronautical field, but it requires some subtle mathematical tools that we will quickly recall.
	\item \textit{Dynamical shape design of a solid inside a fluid flow}: the novelty compared to the last item is that the shape is moving \& we are looking for the best evolution of this shape that both satisfies some geometrical constraints \& optimizes some cost functionals. This is a rather natural technique in order to control a fluid flow pattern, but still its design requires some new mathematical tools that will be sketched in this introduction \& more detailed in the core of this lecture note.
	\item \textit{Control of an elastic solid inside a fluid flow}: this is the most complex \& most realistic situation where both the fluid \& the solid have their own dynamics which are coupled through the fluid-solid interface. Then, we would like to control or optimize the behavior of this coupled system thanks to boundary conditions. The mathematical analysis of this situation uses the whole framework introduced previously. This is a challenging problem, both on the mathematical point of view \& on the technological side. The goal of this book is to partially answer to some issues related to this problem.'' -- \cite[Chap. 1, Sect. 1.4, pp. 7--8]{Moubachir_Zolesio2006}
\end{enumerate}
\paragraph{The objective functional.} ``A common topic in the optimization \& control field of PDE systems is the choice of appropriate cost functionals, i.e., meeting both our objectives \& the mathematical requirements that guarantee the convergence to at least 1 optimum parameter. This functional can depend both on the state variables $({\bf u},p)$ \& on the control parameter ${\bf g}$.'' [$\ldots$] ``More generally, we can consider any cost functionals that are twice-differentiable w.r.t. their arguments.''-- \cite[Chap. 1, Subsect. 1.4.1, pp. 9--10]{Moubachir_Zolesio2006}

\paragraph{The control problem.} ``Our goal is now furnish the 1st-order optimality conditions associated to the optimization problem. These conditions are very useful since they are the basis in order to build both a rigorous mathematical analysis \& gradient-based optimization algorithms.

There exists 2 main methods in order to derive these conditions: the 1st one is based on the differentiability of the state variables w.r.t. the control parameter \& the 2nd one relies on the existence of Lagrangian multipliers.'' -- \cite[Chap. 1, Subsect. 1.4.1, p. 10]{Moubachir_Zolesio2006}

\paragraph{Sensitivity.} ``Let us consider a \textit{control point} ${\bf g}\in\mathcal{U}$, then the cost functional $j({\bf g})$ is Fr\'echet differentiable w.r.t. ${\bf g}$ \cite{Abergel_Temam1990}, [71] \& its directional derivative is given by \textbf{(1.9)}
\begin{align*}
	\langle j'({\bf g}),{\bf h}\rangle = \langle\partial_{({\bf u},p)}J[({\bf u},p)({\bf g})],({\bf u}',p')({\bf g};{\bf h})\rangle,\mbox{ where }({\bf u}',p')({\bf g};{\bf h})\coloneqq\frac{d}{d{\bf g}}({\bf u},p)({\bf g})\cdot{\bf h}
\end{align*}
stands for the directional derivative of $({\bf u},p)({\bf g})$ w.r.t. ${\bf g}$.'' [$\ldots$]

``Then the 1st-order optimality condition writes \textbf{(1.11)} $\langle j'({\bf g},{\bf h})\rangle = 0$, $\forall{\bf h}\in\mathcal{U}$. I.e., the set of optimal controls is contained in the set of critical points for the cost function $j({\bf g})$. However, we would like to obtain an expression of this condition avoiding the direction ${\bf h}\in\mathcal{U}$. To this end, we introduce the \textit{adjoint variable} $({\bf v},\pi)$ solution of the \textit{adjoint linearized Navier--Stokes system} \textbf{(1.12)}. Consequently, we are able to identify the gradient of the cost function as the trace on $\Gamma^c$ of the \textit{adjoint normal stress tensor}, i.e., \textbf{(1.13)}
\begin{align*}
	\nabla j({\bf g}) = {}^\star\gamma_{(0,\tau)\times\Gamma^c}[\sigma({\bf v},\pi)\cdot{\bf n}].
\end{align*}
This formal proof provides the basic steps needed in order to build a gradient-based optimization method associated to the control problem $\min_{{\bf g}\in\mathcal{U}} j({\bf g})$.

An alternative approach consists in avoiding the derivation of the fluid state $({\bf u},p)$ w.r.t. the control ${\bf g}$ thanks to the introduction of a Lagrangian functional that includes not only the cost functional but also the state equation, \textbf{(1.14)}
\begin{align*}
	\mathcal{L}(\boldsymbol{\psi},r,\boldsymbol{\phi},q;{\bf g}) = J(\boldsymbol{\psi},r) + \langle e(\boldsymbol{\psi},r;{\bf g}),(\boldsymbol{\phi},q)\rangle,
\end{align*}
where $\langle e({\bf u},p;g),(\boldsymbol{\phi},q)\rangle$ stands for the weak form of the state equation NSEs (1.8), e.g.,
\begin{align*}
	\langle e(\boldsymbol{\psi},r;{\bf g}),(\boldsymbol{\phi},q)\rangle =&\, \int_{(0,\tau)\times\Omega^f} [-\boldsymbol{\psi}\cdot\partial_t\boldsymbol{\phi} + ({\rm D}\boldsymbol{\psi}\cdot\boldsymbol{\psi})\cdot\boldsymbol{\phi} - \nu\boldsymbol{\psi}\cdot\Delta\boldsymbol{\phi} + \boldsymbol{\psi}\cdot\nabla q - r\nabla\cdot\boldsymbol{\phi}] + \int_{(0,\tau)\times\Gamma^c} {\bf g}\cdot(\sigma(\boldsymbol{\phi},q)\cdot{\bf n})\,{\rm d}\Gamma\,{\rm d}t\\
	&+ \int_{(0,\tau)\times\partial D} {\bf u}_\infty\cdot(\sigma(\boldsymbol{\phi},q)\cdot{\bf n}) + \int_{\Omega^f} \boldsymbol{\psi}(\tau)\cdot\boldsymbol{\phi}(\tau) - \int_{\Omega^f} {\bf u}_0\cdot\boldsymbol{\phi}(0).
\end{align*}
Hence the control problem $\min_{{\bf g}\in\mathcal{U}} j({\bf g})$ is equivalent to the $\min$-$\max$ problem, \textbf{(1.15)}
\begin{align*}
	\min_{{\bf g}\in\mathcal{U}}\min_{(\boldsymbol{\psi},r)}\max_{(\boldsymbol{\phi},q)} \mathcal{L}(\boldsymbol{\psi},r,\boldsymbol{\phi},q;{\bf g}).
\end{align*}
For every control ${\bf g}\in\mathcal{U}$, it can be proven that the $\min$-$\max$ problem,
\begin{align*}
	\min_{(\boldsymbol{\psi},r)}\max_{(\boldsymbol{\phi},q)} \mathcal{L}(\boldsymbol{\psi},r,\boldsymbol{\phi},q;{\bf g})
\end{align*}
admits a unique saddle-point $({\bf u},p;{\bf v},\pi)$ which are solutions of the systems (1.8)--(1.12). Finally the 1st-order optimality for the problem (1.15) writes \textbf{(1.16)}
\begin{align*}
	\partial_{\bf g}\mathcal{L}({\bf u},p,{\bf v},\pi;{\bf g}) = 0
\end{align*}
which turns out to be equivalent to (1.13). Then, we can think to solve the optimality condition (1.11), using a continuous iterative method. Indeed let us introduce a scalar parameter $s\ge 0$, \& a \textit{control variable} ${\bf g}(s)$ that is differentiable w.r.t. $s$. Hence using the differentiability of $J({\bf g})$, we get
\begin{align*}
	J({\bf g}(r)) - J({\bf g}(0)) = \int_0^r \langle\nabla J({\bf g}(s)),{\bf g}'(s)\rangle_{\mathcal{U}^\star,\mathcal{U}}\,{\rm d}s.
\end{align*}
Let us choose the control s.t. \textbf{(1.17)}
\begin{align*}
	{\bf g}'(s) + \mathbb{A}^{-1}(s)\nabla J({\bf g}(s)) = 0,\ s\in(0,r),
\end{align*}
where $\mathbb{A}$ stands for an appropriate duality operator, then the functional writes
\begin{align*}
	J({\bf g}(r)) - J({\bf g}(0)) = -\int_0^r |\nabla J({\bf g}(s))|^2\,{\rm d}s.
\end{align*}
I.e., the control law (1.17) leads to a functional's decrease \& is referred to as a continuous gradient based optimization method. Using a discretization of the parameter $s$ leads to a standard gradient-based method such as the conjugate-gradient or the quasi-Newton method depending on the choice of $\mathbb{A}(s)$.'' -- \cite[Chap. 1, Subsect. 1.4.1, pp. 11--12]{Moubachir_Zolesio2006}

``We again consider the situation where a fixed solid is surrounded by a fluid flow. The shape control consists in finding the optimal shape of the solid that reduces some objective functional (e.g., the drag) under some perimeter, volume or curvature constraints. This optimization is an open-loop control since the shape of the obstacle is time-independent.''

\paragraph{The speed method.} ``Here the space of shapes is no more a linear space \& the associated differential calculus becomes more tricky. Our goal is to build \textit{gradient-based methods} in order to find the optimal shape, i.e., we would like to solve the following problem, \textbf{(1.18)} $\min_{\Omega\in\mathcal{A}} J(\Omega)$. In order to carry out the sensitivity analysis of functionals depending on the shape of the solid $\Omega$, we introduce a family of pertubated domains $\Omega_s\subset D$ parametrized by a scholar parameter $0\le s\le\varepsilon$. These domains are the images of the original domain $\Omega$ through a given family of smooth maps\footnote{Typically we have the following Lipschitz regularity assumptions:
\begin{align*}
	{\bf T}(\cdot,{\bf x})&\in\mathcal{C}^1([0,\varepsilon];\mathbb{R}^3),\ \forall{\bf x}\in D,\ \|{\bf T}(\cdot,{\bf x}) - {\bf T}(\cdot,{\bf y})\|_{\mathcal{C}^0([0,\varepsilon];\mathbb{R}^3)}\le C\|{\bf x} - {\bf y}\|_{\mathbb{R}^3},\\
	{\bf T}^{-1}(\cdot,{\bf x})&\in\mathcal{C}^0([0,\varepsilon];\mathbb{R}^3),\ \forall{\bf x}\in D,\ \|{\bf T}^{-1}(\cdot,{\bf x}) - {\bf T}^{-1}(\cdot,{\bf y})\|_{\mathcal{C}^0([0,\varepsilon];\mathbb{R}^3)}\le C\|{\bf x} - {\bf y}\|_{\mathbb{R}^3},
\end{align*}
where $C > 0$.} ${\bf T}_s:\overline{D}\to\overline{D}$, i.e. $\Omega_s = {\bf T}_s(\Omega)$, $\Gamma_s = {\bf T}_s(\Gamma)$.

2 major classes of such mappings are given by:
\begin{itemize}
	\item the \textit{identity perturbation method} ([116, 125]), ${\bf T}_s = {\rm I} + s\boldsymbol{\theta}$, where $\boldsymbol{\theta}:\overline{D}\to\overline{D}$.
	\item the \textit{speed method} [145] \cite{Pironneau1984}, where the transformation is the flow associated to a given velocity field ${\bf V}(s,{\bf x})$,
	\begin{equation*}
		\left\{\begin{split}
			\partial_s{\bf T}_s({\bf x}) &= {\bf V}(s,{\bf T}_s({\bf x})),&&(s,{\bf x})\in(0,\varepsilon)\times D,\\
			{\bf T}_{s=0}({\bf x}) &= {\bf x},&&{\bf x}\in D.
		\end{split}\right.
	\end{equation*}
	In order for $\overline{D}$ to be globally invariant under ${\bf T}_s({\bf V})$ we need to impose the following \textit{viability conditions}, ${\bf V}(s,{\bf x})\cdot{\bf n}({\bf x}) = 0$, ${\bf x}\in\partial D$.
\end{itemize}
Let us consider the family of functionals $J(\Omega_s)$ that depends on the shapes $\Omega_s$, e.g., the work to overcome the drag exerted by the fluid on the solid boundary, \textbf{(1.19)}
\begin{align*}
	J_{\rm drag}(\Omega) = \int_{(0,\tau)\times\Gamma} ({\bf u} - {\bf u}_\infty)\cdot\sigma({\bf u},p)\cdot{\bf n}\,{\rm d}\Gamma\,{\rm d}t.
\end{align*}
This functional depends on $\Omega$ not only because it is an integral over the boundary $\Gamma$, but also because it involves the solution $({\bf u},p)$ of the Navier--Stokes system, \textbf{(1.20)}, that depends on $\Omega$.

To perform our sensitivity analysis, we choose to work in the framework of the \textit{speed method}\footnote{which leads, at least for the 1st order terms, to the same results as the identity perturbation framework [51].} We define the Eulerian derivative of the shape functional $J(\Omega)$ at point $\Omega$ in the direction of the vector field ${\bf V}\in\mathcal{V}$ as the limit,
\begin{align*}
	dJ(\Omega;{\bf V}) = \lim_{s\downarrow 0} \frac{J(\Omega_s({\bf V})) - J(\Omega)}{s},
\end{align*}
where $\mathcal{V}$ is a linear space\footnote{e.g., $\mathcal{V}\coloneqq\{{\bf V}\in\mathcal{C}^0(0,\varepsilon;\mathcal{C}^1(D;\mathbb{R}^3)),\ \nabla\cdot{\bf V} = 0\mbox{ in } D,\ \langle{\bf V},{\bf n}\rangle = 0\mbox{ on }\partial D\}$.} If this limit exists \& is finite $\forall{\bf V}\in\mathcal{V}$ \& the mapping $\mathcal{V}\to\mathbb{R}$, ${\bf V}\mapsto dJ(\Omega;{\bf V})$ is linear \& continuous, then the functional $J(\Omega)$ is said to be \textit{shape differentiable}.

'' -- \cite[Chap. 1, Subsect. 1.4.2, pp. 13--]{Moubachir_Zolesio2006}

%------------------------------------------------------------------------------%

\chapter{Topology Optimization}

%------------------------------------------------------------------------------%

\printbibliography[heading=bibintoc]
	
\end{document}